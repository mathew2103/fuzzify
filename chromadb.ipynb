{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-k_9tRICFWRh"
   },
   "source": [
    "# Installation:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYIXdlYsm0Yu"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "!pip install chromadb epitran panphon numpy phonemizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c7ZCesnnSWt"
   },
   "source": [
    "## To install epitran dependencies:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb epitran panphon numpy\n",
    "# Install espeak-ng based on the operating system\n",
    "import platform\n",
    "from subprocess import check_output, CalledProcessError\n",
    "if platform.system() == \"Linux\":\n",
    "    !apt-get update\n",
    "    !apt-get install -y espeak-ng\n",
    "elif platform.system() == \"Windows\":\n",
    "    try:\n",
    "        check_output(['choco', 'install', 'espeak-ng'], text=True)\n",
    "    except CalledProcessError as e:\n",
    "        print(f\"Error installing espeak-ng: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "mw6s7DKV2cIe",
    "outputId": "33619ab0-5b95-40f3-99d8-15f28e5bad25"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y build-essential git wget\n",
    "# Clone the Flite repository\n",
    "!git clone https://github.com/festvox/flite.git\n",
    "%cd flite\n",
    "# Build Flite\n",
    "!./configure && make\n",
    "# Build lex_lookup\n",
    "%cd testsuite\n",
    "!make lex_lookup\n",
    "# Move lex_lookup to a global path\n",
    "!sudo cp lex_lookup /usr/local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Vdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qcp8eFBMGFsM",
    "outputId": "ae1881fd-6398-476d-f6fa-e9c42cc4dc11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Vdb\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "b0bhvR0JGGfB",
    "outputId": "a70c4f11-cce5-4eb7-89e7-541c8971d930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8U5nSUgGRZN"
   },
   "source": [
    "# To IPA conversion:-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) via epitran(flite backend):-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vd4uzfcR2Vbc"
   },
   "outputs": [],
   "source": [
    "from epitran import Epitran\n",
    "\n",
    "# Initialize Epitran for English\n",
    "english_epi = Epitran('eng-Latn')\n",
    "\n",
    "def english_to_ipa(text):\n",
    "    \"\"\"Converts English text to IPA using Epitran.\"\"\"\n",
    "    try:\n",
    "        return english_epi.transliterate(text)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Initialize Epitran for Hindi\n",
    "hindi_epi = Epitran('hin-Deva')\n",
    "\n",
    "def hindi_to_ipa(text):\n",
    "    \"\"\"Converts Hindi text to IPA using Epitran.\"\"\"\n",
    "    try:\n",
    "        return hindi_epi.transliterate(text)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ɑɹθi\n"
     ]
    }
   ],
   "source": [
    "print(english_to_ipa(\"aarthy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) via phonemizer(espeak backend):-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonemizer import phonemize\n",
    "\n",
    "def english_to_ipa(text):\n",
    "    \"\"\"Converts English text to IPA using phonemiser with espeak-ng.\"\"\"\n",
    "    try:\n",
    "        ipa = phonemize(text, language='en-us', backend='espeak', strip=True)\n",
    "        return ipa\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def hindi_to_ipa(text):\n",
    "    \"\"\"Converts Hindi text to IPA using epitran.\"\"\"\n",
    "    try:\n",
    "        return hindi_epi.transliterate(text)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvbqhr1M5kl7",
    "outputId": "51b711a2-dca0-4c81-f522-e48d35a9a570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ɑɹθi\n"
     ]
    }
   ],
   "source": [
    "print(english_to_ipa(\"aarthy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oroXviy4eD3o"
   },
   "source": [
    "# Database actions:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9_I7SwtEdVsw"
   },
   "outputs": [],
   "source": [
    "import panphon\n",
    "import panphon.distance \n",
    "import numpy as np\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "import chromadb\n",
    "import os\n",
    "# Initialize PanPhon\n",
    "ft = panphon.FeatureTable()\n",
    "from panphon import distance\n",
    "dst = distance.Distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alt_ipa import alt_ipa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OrCOu3tXd63p"
   },
   "outputs": [],
   "source": [
    "# Specify the persistence directory\n",
    "persist_directory = \"Vdb(epi)\"\n",
    "os.makedirs(persist_directory, exist_ok=True)\n",
    "client = chromadb.PersistentClient(path=persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# def create_dataset_from_excel(file_path, sheet_name=0):\n",
    "#     \"\"\"\n",
    "#     Reads an Excel file and creates a dataset for ChromaDB.\n",
    "\n",
    "#     Args:\n",
    "#     - file_path (str): Path to the Excel file.\n",
    "#     - sheet_name (str or int): Sheet name or index to read from the Excel file.\n",
    "\n",
    "#     Returns:\n",
    "#     - names (list): List of names from the Excel file.\n",
    "#     - ipas (list): List of IPA representations of the names.\n",
    "#     \"\"\"\n",
    "#     # Read the Excel file\n",
    "#     df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "#     # Assuming the Excel file has a column named 'Name'\n",
    "#     names = df['Name'].tolist()\n",
    "\n",
    "#     # Convert names to IPA\n",
    "#     ipas = [english_to_ipa(name) for name in names]\n",
    "\n",
    "#     return names, ipas\n",
    "\n",
    "# # Example usage\n",
    "# file_path = 'path_to_your_excel_file.xlsx'\n",
    "# names, ipas = create_dataset_from_excel(file_path)\n",
    "\n",
    "# # Add pronunciations to the ChromaDB collection\n",
    "# embedder(names, ipas)\n",
    "# # Create a new ChromaDB collection\n",
    "# new_collection = client.create_collection(name=\"new_excel_collection\")\n",
    "\n",
    "# Add pronunciations to the new ChromaDB collection\n",
    "# embedder(names, ipas)\n",
    "# Add all values of the Excel file to the collection\n",
    "def add_excel_data_to_collection(file_path, sheet_name=0):\n",
    "    \"\"\"\n",
    "    Reads an Excel file and adds data to the ChromaDB collection.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the Excel file.\n",
    "    - sheet_name (str or int): Sheet name or index to read from the Excel file.\n",
    "    \"\"\"\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Assuming the Excel file has a column named 'Name'\n",
    "    names = df['Name'].tolist()\n",
    "\n",
    "    # Convert names to IPA\n",
    "    ipas = [english_to_ipa(name) for name in names]\n",
    "\n",
    "    # Prepare metadata\n",
    "    metadatas = df.to_dict(orient='records')\n",
    "\n",
    "    # Prepare embeddings\n",
    "    embeddings = [ipa2vec(ipa) for ipa in pronunciationipas]\n",
    "    # embeddings = [ft.word_array(ipa) for ipa in ipas]\n",
    "    # Add data to the collection\n",
    "    collection.add(\n",
    "        documents=names,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=metadatas,\n",
    "        ids=[str(i) for i in range(len(names))]\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "add_excel_data_to_collection(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoBI6U9EWNI0"
   },
   "source": [
    "## To list collections:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_x3tWZF2WKxR",
    "outputId": "8c810da3-28e1-4447-9cee-bd243d967a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection(name=200k_single_hindi_cosine_today)\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all collection names\n",
    "collection_names = client.list_collections()\n",
    "\n",
    "# Print the collection names\n",
    "for name in collection_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mR1imrZJdpjJ"
   },
   "source": [
    "## To create a new vdb:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "id": "O28R00YudmLP"
   },
   "outputs": [],
   "source": [
    "# Create a ChromaDB collection\n",
    "# collection = client.create_collection( name=\"vdb_l2\")\n",
    "collection = client.create_collection( name=\"200k_single_hindi_cosine_today\",metadata={\"hnsw:space\": \"cosine\"} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvnIDJblWVn_"
   },
   "source": [
    "## To delete a collection:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yi1YHroMSzbG"
   },
   "outputs": [],
   "source": [
    "client.delete_collection(name=\"vdb_fuzzified56k-200k_cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9YuNNmnduJv"
   },
   "source": [
    "## or load an existing db:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final db:-\n",
    "collection = client.get_collection(\"200k_single_hindi_cosine_today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(\"final_200k_fullname_cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "b1595_rHdjVW"
   },
   "outputs": [],
   "source": [
    "collection = client.get_collection(\"vdb_l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "RV1p1T1wZmQp"
   },
   "outputs": [],
   "source": [
    "collection = client.get_collection(\"vdb_cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vo1rdgT9ZmDV"
   },
   "outputs": [],
   "source": [
    "collection = client.get_collection(\"vdb_277k_fullname_cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents in the collection: 0\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of documents in the collection\n",
    "total_documents = collection.count()\n",
    "print(f\"Total number of documents in the collection: {total_documents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "th5u4v96eaDq"
   },
   "source": [
    "## Function definitions:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "l8K3fLso2cz0"
   },
   "outputs": [],
   "source": [
    "# Function to compute the average feature vector (same as before)\n",
    "def ipa2vec(ipa):\n",
    "    vectors = ft.word_to_vector_list(ipa, numeric=True)\n",
    "    processed_vectors = np.array(vectors)\n",
    "    if processed_vectors.ndim == 2:\n",
    "        avg_vector = np.mean(processed_vectors, axis=0)\n",
    "    else:\n",
    "        avg_vector = processed_vectors\n",
    "    return avg_vector.tolist()  # ChromaDB needs lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average feature vector (same as before)\n",
    "def cipa2vec(ipa):\n",
    "    vectors = ft.word_to_vector_list(ipa, numeric=True)\n",
    "    processed_vectors = np.array(vectors)\n",
    "    if processed_vectors.ndim == 2:\n",
    "        avg_vector = np.concatenate(processed_vectors, axis=0)\n",
    "    else:\n",
    "        avg_vector = processed_vectors\n",
    "    return avg_vector.tolist()  # ChromaDB needs lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nowɛl\n",
      "[[-1, 1, 1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0], [1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 0, -1, 0, -1, -1, -1, 1, 1, -1, 1, -1, 0, 0], [-1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 0, 1, 1, -1, 1, 1, -1, 0, -1, 0, 0], [1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0], [-1, 1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0]]\n",
      "24\n",
      "[-1, 1, 1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 0, -1, 0, -1, -1, -1, 1, 1, -1, 1, -1, 0, 0, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 0, 1, 1, -1, 1, 1, -1, 0, -1, 0, 0, 1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0]\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "ipa=english_to_ipa(\"noel\")\n",
    "print(ipa)\n",
    "print(ft.word_to_vector_list(ipa, numeric=True))\n",
    "print(len(ft.word_to_vector_list(ipa, numeric=True)[0]))\n",
    "print(cipa2vec(english_to_ipa(\"noel\")))\n",
    "print(len(cipa2vec(english_to_ipa(\"noel\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embedder(names, pronunciations):\n",
    "    \"\"\"Adds pronunciations to the ChromaDB collection.\"\"\"\n",
    "\n",
    "    if len(names) != len(pronunciations):\n",
    "        raise ValueError(\"Names and pronunciations lists must have the same length.\")\n",
    "\n",
    "    embeddings = [ipa2vec(ipa) for ipa in pronunciations]\n",
    "    collection.add(\n",
    "        documents=names,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=[{\"ipa\": ipa} for ipa in pronunciations],  # Store IPA for reference\n",
    "        ids=[str(i) for i in range(len(names))] # provide IDs to avoid issues\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents in the collection: 404845\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of documents in the collection\n",
    "total_documents = collection.count()\n",
    "print(f\"Total number of documents in the collection: {total_documents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querrier:-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without weighted levenshtien:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chromaquerrier(query_ipa, n_results=5):\n",
    "    \"\"\"Retrieves similar names from the ChromaDB vdb.\"\"\"\n",
    "    query_embedding = ipa2vec(query_ipa)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=[\"distances\", \"metadatas\", \"documents\"] #include additional data\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yc1jiD-0ZXqp"
   },
   "source": [
    "##  with weighted levenshtein edit distance:-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with alt ipa:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    \"\"\"Detects if the input text is Hindi or English.\"\"\"\n",
    "    if any('\\u0900' <= char <= '\\u097F' for char in text):\n",
    "        return 'hindi'\n",
    "    else:\n",
    "        return 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नोएल\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Initialize the Translator\n",
    "translator = Translator()\n",
    "\n",
    "def trans_hindi_to_english(hindi_text):\n",
    "    result = translator.translate(hindi_text, src='hi', dest='en')\n",
    "    return result.text\n",
    "\n",
    "def trans_english_to_hindi(english_text):\n",
    "    result = translator.translate(english_text, src='en', dest='hi')\n",
    "    return result.text\n",
    "print(trans_english_to_hindi(\"noel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querrier(name, n_results=50, weight_chromadb=0.7, weight_edit_distance=0.3, cutoff=0.6):\n",
    "    \"\"\"Retrieves similar names with combined ChromaDB and edit distance scoring.\"\"\"\n",
    "    language = detect_language(name)\n",
    "    if language == 'hindi':\n",
    "        query_ipa = hindi_to_ipa(name)\n",
    "        alt_ipa_list = alt_ipa(query_ipa)\n",
    "    else:\n",
    "        query_ipa = english_to_ipa(name)\n",
    "        alt_ipa_list = alt_ipa(name)\n",
    "    print(f\"alt_ipa_list: {alt_ipa_list}\")\n",
    "    all_scores = []\n",
    "\n",
    "    # Process the main query IPA first:-\n",
    "    main_query_embedding = ipa2vec(query_ipa)\n",
    "    main_results = collection.query(\n",
    "        query_embeddings=[main_query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=[\"distances\", \"metadatas\", \"documents\"]\n",
    "    )\n",
    "    for i in range(len(main_results[\"documents\"][0])):\n",
    "        chromadb_distance = main_results[\"distances\"][0][i]\n",
    "        metadata = main_results[\"metadatas\"][0][i]\n",
    "        doc_id = main_results[\"ids\"][0][i]\n",
    "        edit_distance = dst.weighted_feature_edit_distance(query_ipa, metadata[\"ipa\"])\n",
    "        combined_score = weight_chromadb * chromadb_distance + weight_edit_distance * edit_distance\n",
    "        # if combined_score > cutoff:\n",
    "        #     continue  # Skip results with a combined score greater than cutoff\n",
    "        all_scores.append((main_results[\"documents\"][0][i], metadata, chromadb_distance, combined_score, doc_id))\n",
    "        print(f\"Document: {main_results['documents'][0][i]}, ChromaDB Distance: {chromadb_distance}, Combined Score: {combined_score}, Weighted Edit Distance: {edit_distance}, ID: {doc_id}\")\n",
    "    # Process alternate IPAs:-\n",
    "    weight_edit_distance=0\n",
    "    weight_chromadb=1\n",
    "    cutoff=0.003\n",
    "    for alt_ipa_item in alt_ipa_list:\n",
    "        alt_query_embedding = ipa2vec(alt_ipa_item)\n",
    "        alt_results = collection.query(\n",
    "            query_embeddings=[alt_query_embedding],\n",
    "            n_results=n_results,\n",
    "            include=[\"distances\", \"metadatas\", \"documents\"]\n",
    "        )\n",
    "        for i in range(len(alt_results[\"documents\"][0])):\n",
    "            chromadb_distance = alt_results[\"distances\"][0][i]\n",
    "            metadata = alt_results[\"metadatas\"][0][i]\n",
    "            edit_distance = dst.weighted_feature_edit_distance(alt_ipa_item, metadata[\"ipa\"])\n",
    "            combined_score = weight_chromadb * chromadb_distance + weight_edit_distance * edit_distance\n",
    "            # if combined_score > cutoff:\n",
    "            #     continue  # Skip results with a combined score greater than cutoff\n",
    "            all_scores.append((alt_results[\"documents\"][0][i], metadata, chromadb_distance, combined_score))\n",
    "            print(f\"altDocument: {alt_results['documents'][0][i]}, ChromaDB Distance: {chromadb_distance}, Combined Score: {combined_score}, Weighted Edit Distance: {edit_distance}\")\n",
    "    # Sort all combined results by chromadb_distance (ascending order)\n",
    "    all_scores.sort(key=lambda item: item[2])  # Sort by chromadb_distance\n",
    "\n",
    "    # Extract top results\n",
    "    filtered_results = {\n",
    "        \"documents\": [item[0] for item in all_scores],\n",
    "        \"metadatas\": [item[1] for item in all_scores],\n",
    "        \"distances\": [item[2] for item in all_scores],\n",
    "        \"ids\": [item[4] for item in all_scores],\n",
    "    }\n",
    "    # Remove duplicate elements in filtered_results:-\n",
    "    unique_documents = []\n",
    "    unique_metadatas = []\n",
    "    unique_distances = []\n",
    "    unique_ids = []\n",
    "\n",
    "    for doc, meta, dist, doc_id in zip(filtered_results[\"documents\"], filtered_results[\"metadatas\"], filtered_results[\"distances\"], filtered_results[\"ids\"]):\n",
    "        if doc not in unique_documents:\n",
    "            unique_documents.append(doc)\n",
    "            unique_metadatas.append(meta)\n",
    "            unique_distances.append(dist)\n",
    "            unique_ids.append(doc_id)\n",
    "\n",
    "    filtered_results[\"documents\"] = unique_documents\n",
    "    filtered_results[\"metadatas\"] = unique_metadatas\n",
    "    filtered_results[\"distances\"] = unique_distances\n",
    "    filtered_results[\"ids\"] = unique_ids\n",
    "    return filtered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wihtout alt ipa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querrier(name, n_results=100, weight_chromadb=0.7, weight_edit_distance=0.3, cutoff=0.6):\n",
    "    \"\"\"Retrieves similar names with combined ChromaDB and edit distance scoring.\"\"\"\n",
    "    language = detect_language(name)\n",
    "    if language == 'hindi':\n",
    "        query_ipa = hindi_to_ipa(name)\n",
    "        print(query_ipa)\n",
    "        # alt_ipa_list = alt_ipa(query_ipa)\n",
    "    else:\n",
    "        query_ipa = english_to_ipa(name)\n",
    "        # alt_ipa_list = alt_ipa(name)\n",
    "    # print(f\"alt_ipa_list: {alt_ipa_list}\")\n",
    "    all_scores = []\n",
    "\n",
    "    # Process the main query IPA first:-\n",
    "    main_query_embedding = ipa2vec(query_ipa)\n",
    "    main_results = collection.query(\n",
    "        query_embeddings=[main_query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=[\"distances\", \"metadatas\", \"documents\"]\n",
    "    )\n",
    "    for i in range(len(main_results[\"documents\"][0])):\n",
    "        chromadb_distance = main_results[\"distances\"][0][i]\n",
    "        metadata = main_results[\"metadatas\"][0][i]\n",
    "        doc_id = main_results[\"ids\"][0][i]\n",
    "        edit_distance = dst.weighted_feature_edit_distance(query_ipa, metadata[\"ipa\"])\n",
    "        combined_score = weight_chromadb * chromadb_distance + weight_edit_distance * edit_distance\n",
    "        if combined_score > cutoff:\n",
    "            continue  # Skip results with a combined score greater than cutoff\n",
    "        all_scores.append((main_results[\"documents\"][0][i], metadata, chromadb_distance, combined_score, doc_id))\n",
    "        print(f\"Document: {main_results['documents'][0][i]}, ChromaDB Distance: {chromadb_distance}, Combined Score: {combined_score}, Weighted Edit Distance: {edit_distance}, ID: {doc_id}\")\n",
    "    # Process alternate IPAs:-\n",
    "    # weight_edit_distance=0\n",
    "    # weight_chromadb=1\n",
    "    # cutoff=0.003\n",
    "    # for alt_ipa_item in alt_ipa_list:\n",
    "    #     alt_query_embedding = ipa2vec(alt_ipa_item)\n",
    "    #     alt_results = collection.query(\n",
    "    #         query_embeddings=[alt_query_embedding],\n",
    "    #         n_results=n_results,\n",
    "    #         include=[\"distances\", \"metadatas\", \"documents\"]\n",
    "    # #     )\n",
    "    #     for i in range(len(alt_results[\"documents\"][0])):\n",
    "    #         chromadb_distance = alt_results[\"distances\"][0][i]\n",
    "    #         metadata = alt_results[\"metadatas\"][0][i]\n",
    "    #         edit_distance = dst.weighted_feature_edit_distance(alt_ipa_item, metadata[\"ipa\"])\n",
    "    #         combined_score = weight_chromadb * chromadb_distance + weight_edit_distance * edit_distance\n",
    "    #         # if combined_score > cutoff:\n",
    "    #         #     continue  # Skip results with a combined score greater than cutoff\n",
    "    #         all_scores.append((alt_results[\"documents\"][0][i], metadata, chromadb_distance, combined_score))\n",
    "    #         print(f\"altDocument: {alt_results['documents'][0][i]}, ChromaDB Distance: {chromadb_distance}, Combined Score: {combined_score}, Weighted Edit Distance: {edit_distance}\")\n",
    "    # Sort all combined results by chromadb_distance (ascending order)\n",
    "    all_scores.sort(key=lambda item: item[2])  # Sort by chromadb_distance\n",
    "\n",
    "    # Extract top results\n",
    "    filtered_results = {\n",
    "        \"documents\": [item[0] for item in all_scores],\n",
    "        \"metadatas\": [item[1] for item in all_scores],\n",
    "        \"distances\": [item[2] for item in all_scores],\n",
    "        \"ids\": [item[4] for item in all_scores],\n",
    "    }\n",
    "    # Remove duplicate elements in filtered_results:-\n",
    "    unique_documents = []\n",
    "    unique_metadatas = []\n",
    "    unique_distances = []\n",
    "    unique_ids = []\n",
    "\n",
    "    for doc, meta, dist, doc_id in zip(filtered_results[\"documents\"], filtered_results[\"metadatas\"], filtered_results[\"distances\"], filtered_results[\"ids\"]):\n",
    "        if doc not in unique_documents:\n",
    "            unique_documents.append(doc)\n",
    "            unique_metadatas.append(meta)\n",
    "            unique_distances.append(dist)\n",
    "            unique_ids.append(doc_id)\n",
    "\n",
    "    filtered_results[\"documents\"] = unique_documents\n",
    "    filtered_results[\"metadatas\"] = unique_metadatas\n",
    "    filtered_results[\"distances\"] = unique_distances\n",
    "    filtered_results[\"ids\"] = unique_ids\n",
    "    return filtered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQOe5TFlwLsY"
   },
   "source": [
    "# To input data to the database:-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mM1Pwc-LxG4x"
   },
   "source": [
    "## To input English data :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bH6fTuyHBBB"
   },
   "source": [
    "## To enter large data:-(>40k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for multi part names:-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "police_stations = [\n",
    "    # Rewa District\n",
    "    \"Antrela\", \"Baikunthpur\", \"Chakghat\", \"Chorhata\", \"Civil Lines\", \"Dabhora\", \"Garh\",\n",
    "    \"Govindgarh\", \"Hanumana\", \"Java\", \"Kotwali\", \"Laur\", \"Mangwan\", \"Mauganj\",\n",
    "    \"Naigarhi\", \"Panwar\", \"Raipur\", \"Sagra\", \"Semariya\", \"Shahpur\", \"Sirmour\",\n",
    "    \"Sohagi\", \"University\", \"Teonthar\", \"Traffic\", \"Mahila\",\n",
    "    \n",
    "    # Sagar District\n",
    "    \"Kotwali\", \"Motinagar\", \"Cannt\", \"Makronia\", \"Baheria\", \"Civil Lines\", \"Gopal Ganj\",\n",
    "    \"Women Police Station\", \"Banda\", \"Shahgarh\", \"Chhanbila\", \"Behrol\", \"Baryatha\",\n",
    "    \"Binaika\", \"Dalpatpur\", \"SDOP Deori\", \"Deori\", \"Gaurjhawar\", \"Maharajpur\", \"Kesli\",\n",
    "    \"SDOP Khurai\", \"Khurai\", \"Bandri\", \"Malthone\", \"SDOP Rehli\", \"Rehli\", \"Garhakota\",\n",
    "    \"Sanodha\", \"Surkhi\", \"SDOP Rahatgarh\", \"Rahatgarh\", \"Jaisinagar\", \"Naryaoli\",\n",
    "    \"SDOP Bina\", \"Bina\", \"Bhangarh\", \"Khimlasa\", \"Agasod\",\n",
    "\n",
    "    # Indore District\n",
    "    \"Gandhi Nagar\", \"Goutampura\", \"Hatod\", \"Hira Nagar\", \"Juni Indore\", \"Kanadiya\",\n",
    "    \"Khajrana\", \"Khudail\", \"Kishanganj\", \"Kotwali\", \"Kshipra\", \"Lasudia\", \n",
    "    \"Mahila Thana\", \"Malharganj\", \"Manpur\", \"Mhow\", \"MIG\", \"Palasiya\", \n",
    "    \"Pandrinath\", \"Pardeshipura\", \"Rajendra Nagar\", \"Raoji Bazar\", \"Rau\", \n",
    "    \"Sadar Bazar\", \"Sanwer\", \"Sarafa\", \"Sayogita Ganj\", \"Simrol\", \n",
    "    \"Tejaji Nagar\", \"Tilak Nagar\", \"Tukoganj\", \"Vijay Nagar\"\n",
    "]\n",
    "\n",
    "ipc_crimes = [\n",
    "    \"Murder - IPC Section 302\",\n",
    "    \"Attempt to Murder - IPC Section 307\",\n",
    "    \"Culpable Homicide - IPC Section 304\",\n",
    "    \"Rape - IPC Section 376\",\n",
    "    \"Dowry Death - IPC Section 304B\",\n",
    "    \"Kidnapping - IPC Section 361\",\n",
    "    \"Theft - IPC Section 378\",\n",
    "    \"Extortion - IPC Section 383\",\n",
    "    \"Robbery - IPC Section 392\",\n",
    "    \"Dacoity - IPC Section 395\",\n",
    "    \"Criminal Breach of Trust - IPC Section 406\",\n",
    "    \"Cheating - IPC Section 415\",\n",
    "    \"Forgery - IPC Section 463\",\n",
    "    \"Defamation - IPC Section 499\",\n",
    "    \"Causing Grievous Hurt - IPC Section 320\",\n",
    "    \"Criminal Intimidation - IPC Section 506\",\n",
    "    \"Rioting - IPC Section 147\",\n",
    "    \"Unlawful Assembly - IPC Section 141\",\n",
    "    \"Counterfeiting Currency - IPC Section 489A\",\n",
    "    \"Adultery (Decriminalized) - IPC Section 497\",\n",
    "    \"Acid Attack - IPC Section 326A\",\n",
    "    \"Abetment of Suicide - IPC Section 306\",\n",
    "    \"Abduction - IPC Section 362\",\n",
    "    \"Arson - IPC Section 436\",\n",
    "    \"Bigamy - IPC Section 494\",\n",
    "    \"Bribery - IPC Section 171B\",\n",
    "    \"Cruelty to Women - IPC Section 498A\",\n",
    "    \"Embezzlement - IPC Section 403\",\n",
    "    \"False Evidence - IPC Section 191\",\n",
    "    \"Fraud - IPC Section 420\",\n",
    "    \"Harassment - IPC Section 509\",\n",
    "    \"Hurt - IPC Section 319\",\n",
    "    \"Indecent Representation of Women - IPC Section 66E\",\n",
    "    \"Kidnapping for Ransom - IPC Section 364A\",\n",
    "    \"Mischief - IPC Section 425\",\n",
    "    \"Pawning of Stolen Goods - IPC Section 410\",\n",
    "    \"Perjury - IPC Section 191\",\n",
    "    \"Sexual Harassment - IPC Section 354\",\n",
    "    \"Sedition - IPC Section 124A\",\n",
    "    \"Terrorism - IPC Section 121\",\n",
    "    \"Trespass - IPC Section 441\",\n",
    "    \"Unlawful Assembly - IPC Section 141\",\n",
    "    \"Waging War Against the Government - IPC Section 121\",\n",
    "]\n",
    "addresses = [\n",
    "    \"25, Shivaji Marg, Indore, Madhya Pradesh - 452001\",\n",
    "    \"Plot No. 14, Sector C, Govindpura, Bhopal, Madhya Pradesh - 462023\",\n",
    "    \"78, Station Road, Jabalpur, Madhya Pradesh - 482001\",\n",
    "    \"House No. 128, Vijay Nagar, Gwalior, Madhya Pradesh - 474011\",\n",
    "    \"Shop No. 5, MG Road, Ujjain, Madhya Pradesh - 456001\",\n",
    "    \"102, New Market, TT Nagar, Bhopal, Madhya Pradesh - 462003\",\n",
    "    \"56, Patel Nagar, Ratlam, Madhya Pradesh - 457001\",\n",
    "    \"Flat No. 401, Emerald Heights, Indore, Madhya Pradesh - 452010\",\n",
    "    \"Plot No. 89, Shanti Nagar, Rewa, Madhya Pradesh - 486001\",\n",
    "    \"Shop No. 12, Main Bazaar, Satna, Madhya Pradesh - 485001\",\n",
    "    \"House No. 45, Sindhi Colony, Sagar, Madhya Pradesh - 470001\",\n",
    "    \"91, Nehru Ward, Damoh, Madhya Pradesh - 470661\",\n",
    "    \"Shop No. 10, Bus Stand Road, Chhindwara, Madhya Pradesh - 480001\",\n",
    "    \"H.No. 120, Balaghat Road, Seoni, Madhya Pradesh - 480661\",\n",
    "    \"123, Gandhi Nagar, Khargone, Madhya Pradesh - 451001\",\n",
    "    \"Plot No. 35, Narmada Colony, Khandwa, Madhya Pradesh - 450001\",\n",
    "    \"78, Jawahar Chowk, Harda, Madhya Pradesh - 461331\",\n",
    "    \"Flat No. 202, Sai Residency, Hoshangabad, Madhya Pradesh - 461001\",\n",
    "    \"Shop No. 7, Bada Bazaar, Vidisha, Madhya Pradesh - 464001\",\n",
    "    \"Plot No. 56, Saraswati Nagar, Raisen, Madhya Pradesh - 464551\",\n",
    "    \"House No. 22, Shastri Colony, Betul, Madhya Pradesh - 460001\",\n",
    "    \"Plot No. 90, Navjeevan Colony, Dewas, Madhya Pradesh - 455001\",\n",
    "    \"Shop No. 18, Gandhi Chowk, Mandsaur, Madhya Pradesh - 458001\",\n",
    "    \"H.No. 304, Shanti Vihar, Neemuch, Madhya Pradesh - 458441\",\n",
    "    \"Plot No. 65, Shankar Nagar, Morena, Madhya Pradesh - 476001\",\n",
    "    \"Flat No. 102, Galaxy Heights, Bhind, Madhya Pradesh - 477001\",\n",
    "    \"House No. 78, Ram Nagar, Tikamgarh, Madhya Pradesh - 472001\",\n",
    "    \"Plot No. 45, Laxmi Nagar, Chhatarpur, Madhya Pradesh - 471001\",\n",
    "    \"Shop No. 3, Main Road, Panna, Madhya Pradesh - 488001\",\n",
    "    \"House No. 110, Saraswati Colony, Shahdol, Madhya Pradesh - 484001\",\n",
    "    \"Plot No. 76, Green Valley, Anuppur, Madhya Pradesh - 484444\",\n",
    "    \"H.No. 55, Jai Stambh Chowk, Singrauli, Madhya Pradesh - 486889\",\n",
    "    \"Shop No. 21, Market Road, Umaria, Madhya Pradesh - 484661\",\n",
    "    \"Plot No. 13, Industrial Area, Katni, Madhya Pradesh - 483501\",\n",
    "    \"H.No. 99, Surya Nagar, Balaghat, Madhya Pradesh - 481001\",\n",
    "    \"Shop No. 8, Collectorate Road, Mandla, Madhya Pradesh - 481661\",\n",
    "    \"Plot No. 120, Malviya Nagar, Sehore, Madhya Pradesh - 466001\",\n",
    "    \"Flat No. 405, Harmony Apartments, Ashoknagar, Madhya Pradesh - 473331\",\n",
    "    \"Shop No. 11, Rajwada, Datia, Madhya Pradesh - 475661\",\n",
    "    \"House No. 67, Kamla Nagar, Shivpuri, Madhya Pradesh - 473551\"\n",
    "]\n",
    "common_indian_surnames = [\n",
    "    \"Agarwal\", \"Ahuja\", \"Anand\", \"Apte\", \"Arya\", \"Asthana\", \"Bachchan\", \"Bagchi\", \"Bajaj\", \"Balakrishnan\",\n",
    "    \"Banerjee\", \"Bansal\", \"Barua\", \"Basu\", \"Batla\", \"Bedi\", \"Behera\", \"Bhalla\", \"Bhandari\", \"Bhat\",\n",
    "    \"Bhatia\", \"Bhatt\", \"Bhattacharya\", \"Biswas\", \"Bose\", \"Chakrabarti\", \"Chanda\", \"Chandra\", \"Chatterjee\",\n",
    "    \"Chaudhary\", \"Chauhan\", \"Chhabra\", \"Chopra\", \"Dalal\", \"Das\", \"Dash\", \"Datta\", \"Desai\", \"Deshmukh\",\n",
    "    \"Devadiga\", \"Dey\", \"Dhaliwal\", \"Dhar\", \"Dhillon\", \"Dixit\", \"Dubey\", \"Dutta\", \"Gandhi\", \"Garg\",\n",
    "    \"Gaur\", \"Ghosh\", \"Gill\", \"Goel\", \"Gopal\", \"Goswami\", \"Goud\", \"Gowda\", \"Guha\", \"Gupta\", \"Halder\",\n",
    "    \"Iyengar\", \"Iyer\", \"Jain\", \"Jaiswal\", \"Jat\", \"Jha\", \"Joshi\", \"Kadam\", \"Kapoor\", \"Kar\", \"Kashyap\",\n",
    "    \"Kaur\", \"Khan\", \"Khanna\", \"Khare\", \"Khatri\", \"Kochhar\", \"Kohli\", \"Konar\", \"Kulkarni\", \"Kumar\",\n",
    "    \"Lahiri\", \"Lal\", \"Mahajan\", \"Maharaj\", \"Maheshwari\", \"Malhotra\", \"Malik\", \"Mandal\", \"Marathe\",\n",
    "    \"Mehra\", \"Mehta\", \"Meena\", \"Menon\", \"Mishra\", \"Mitra\", \"Mittal\", \"Modi\", \"Mudaliar\", \"Mukherjee\",\n",
    "    \"Nadar\", \"Nag\", \"Nagar\", \"Naidu\", \"Nair\", \"Nanda\", \"Narang\", \"Narayan\", \"Nath\", \"Nayak\", \"Nayyar\",\n",
    "    \"Negi\", \"Nehru\", \"Nigam\", \"Pal\", \"Pande\", \"Pandey\", \"Pant\", \"Parida\", \"Parikh\", \"Patel\", \"Pathak\",\n",
    "    \"Patil\", \"Pawar\", \"Pillai\", \"Pooniwala\", \"Prabhu\", \"Prasad\", \"Puri\", \"Rai\", \"Raj\", \"Rajan\", \"Rajput\",\n",
    "    \"Ram\", \"Raman\", \"Rao\", \"Rastogi\", \"Rathore\", \"Raut\", \"Rawat\", \"Reddy\", \"Sable\", \"Sachdev\", \"Saha\",\n",
    "    \"Sahu\", \"Saini\", \"Salvi\", \"Samaddar\", \"Sandhu\", \"Sankar\", \"Sant\", \"Sapra\", \"Sarangi\", \"Sarkar\",\n",
    "    \"Sarma\", \"Saxena\", \"Sehgal\", \"Sen\", \"Sengupta\", \"Seth\", \"Setty\", \"Shah\", \"Shaikh\", \"Sharma\",\n",
    "    \"Shenoy\", \"Shetty\", \"Shroff\", \"Shukla\", \"Siddhu\", \"Singh\", \"Sinha\", \"Somaiya\", \"Sonawane\", \"Sood\",\n",
    "    \"Srivastava\", \"Subramaniam\", \"Subramanian\", \"Sundaram\", \"Swamy\", \"Talwar\", \"Tandon\", \"Thaker\",\n",
    "    \"Thakur\", \"Thatte\", \"Tiwari\", \"Tripathi\", \"Trivedi\", \"Varshney\", \"Venkatesh\", \"Verma\", \"Yadav\", \"Zachariah\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नोएल\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Initialize the Translator\n",
    "translator = Translator()\n",
    "\n",
    "def trans_hindi_to_english(hindi_text):\n",
    "    result = translator.translate(hindi_text, src='hi', dest='en')\n",
    "    return result.text\n",
    "\n",
    "def trans_english_to_hindi(english_text):\n",
    "    result = translator.translate(english_text, src='en', dest='hi')\n",
    "    return result.text\n",
    "print(trans_english_to_hindi(\"noel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    \"\"\"Detects if the input text is Hindi or English.\"\"\"\n",
    "    if any('\\u0900' <= char <= '\\u097F' for char in text):\n",
    "        return 'hindi'\n",
    "    else:\n",
    "        return 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added batch 1 of 202\n",
      "Added batch 2 of 202\n",
      "Added batch 3 of 202\n",
      "Added batch 4 of 202\n",
      "Added batch 5 of 202\n",
      "Added batch 6 of 202\n",
      "Added batch 7 of 202\n",
      "Added batch 8 of 202\n",
      "Added batch 9 of 202\n",
      "Added batch 10 of 202\n",
      "Added batch 11 of 202\n",
      "Added batch 12 of 202\n",
      "Added batch 13 of 202\n",
      "Added batch 14 of 202\n",
      "Added batch 15 of 202\n",
      "Added batch 16 of 202\n",
      "Added batch 17 of 202\n",
      "Added batch 18 of 202\n",
      "Added batch 19 of 202\n",
      "Added batch 20 of 202\n",
      "Added batch 21 of 202\n",
      "Added batch 22 of 202\n",
      "Added batch 23 of 202\n",
      "Added batch 24 of 202\n",
      "Added batch 25 of 202\n",
      "Added batch 26 of 202\n",
      "Added batch 27 of 202\n",
      "Added batch 28 of 202\n",
      "Added batch 29 of 202\n",
      "Added batch 30 of 202\n",
      "Added batch 31 of 202\n",
      "Added batch 32 of 202\n",
      "Added batch 33 of 202\n",
      "Added batch 34 of 202\n",
      "Added batch 35 of 202\n",
      "Added batch 36 of 202\n",
      "Added batch 37 of 202\n",
      "Added batch 38 of 202\n",
      "Added batch 39 of 202\n",
      "Added batch 40 of 202\n",
      "Added batch 41 of 202\n",
      "Added batch 42 of 202\n",
      "Added batch 43 of 202\n",
      "Added batch 44 of 202\n",
      "Added batch 45 of 202\n",
      "Added batch 46 of 202\n",
      "Added batch 47 of 202\n",
      "Added batch 48 of 202\n",
      "Added batch 49 of 202\n",
      "Added batch 50 of 202\n",
      "Added batch 51 of 202\n",
      "Added batch 52 of 202\n",
      "Added batch 53 of 202\n",
      "Added batch 54 of 202\n",
      "Added batch 55 of 202\n",
      "Added batch 56 of 202\n",
      "Added batch 57 of 202\n",
      "Added batch 58 of 202\n",
      "Added batch 59 of 202\n",
      "Added batch 60 of 202\n",
      "Added batch 61 of 202\n",
      "Added batch 62 of 202\n",
      "Added batch 63 of 202\n",
      "Added batch 64 of 202\n",
      "Added batch 65 of 202\n",
      "Added batch 66 of 202\n",
      "Added batch 67 of 202\n",
      "Added batch 68 of 202\n",
      "Added batch 69 of 202\n",
      "Added batch 70 of 202\n",
      "Added batch 71 of 202\n",
      "Added batch 72 of 202\n",
      "Added batch 73 of 202\n",
      "Added batch 74 of 202\n",
      "Added batch 75 of 202\n",
      "Added batch 76 of 202\n",
      "Added batch 77 of 202\n",
      "Added batch 78 of 202\n",
      "Added batch 79 of 202\n",
      "Added batch 80 of 202\n",
      "Added batch 81 of 202\n",
      "Added batch 82 of 202\n",
      "Added batch 83 of 202\n",
      "Added batch 84 of 202\n",
      "Added batch 85 of 202\n",
      "Added batch 86 of 202\n",
      "Added batch 87 of 202\n",
      "Added batch 88 of 202\n",
      "Added batch 89 of 202\n",
      "Added batch 90 of 202\n",
      "Added batch 91 of 202\n",
      "Added batch 92 of 202\n",
      "Added batch 93 of 202\n",
      "Added batch 94 of 202\n",
      "Added batch 95 of 202\n",
      "Added batch 96 of 202\n",
      "Added batch 97 of 202\n",
      "Added batch 98 of 202\n",
      "Added batch 99 of 202\n",
      "Added batch 100 of 202\n",
      "Added batch 101 of 202\n",
      "Added batch 102 of 202\n",
      "Added batch 103 of 202\n",
      "Added batch 104 of 202\n",
      "Added batch 105 of 202\n",
      "Added batch 106 of 202\n",
      "Added batch 107 of 202\n",
      "Added batch 108 of 202\n",
      "Added batch 109 of 202\n",
      "Added batch 110 of 202\n",
      "Added batch 111 of 202\n",
      "Added batch 112 of 202\n",
      "Added batch 113 of 202\n",
      "Added batch 114 of 202\n",
      "Added batch 115 of 202\n",
      "Added batch 116 of 202\n",
      "Added batch 117 of 202\n",
      "Added batch 118 of 202\n",
      "Added batch 119 of 202\n",
      "Added batch 120 of 202\n",
      "Added batch 121 of 202\n",
      "Added batch 122 of 202\n",
      "Added batch 123 of 202\n",
      "Added batch 124 of 202\n",
      "Added batch 125 of 202\n",
      "Added batch 126 of 202\n",
      "Added batch 127 of 202\n",
      "Added batch 128 of 202\n",
      "Added batch 129 of 202\n",
      "Added batch 130 of 202\n",
      "Added batch 131 of 202\n",
      "Added batch 132 of 202\n",
      "Added batch 133 of 202\n",
      "Added batch 134 of 202\n",
      "Added batch 135 of 202\n",
      "Added batch 136 of 202\n",
      "Added batch 137 of 202\n",
      "Added batch 138 of 202\n",
      "Added batch 139 of 202\n",
      "Added batch 140 of 202\n",
      "Added batch 141 of 202\n",
      "Added batch 142 of 202\n",
      "Added batch 143 of 202\n",
      "Added batch 144 of 202\n",
      "Added batch 145 of 202\n",
      "Added batch 146 of 202\n",
      "Added batch 147 of 202\n",
      "Added batch 148 of 202\n",
      "Added batch 149 of 202\n",
      "Added batch 150 of 202\n",
      "Added batch 151 of 202\n",
      "Added batch 152 of 202\n",
      "Added batch 153 of 202\n",
      "Added batch 154 of 202\n",
      "Added batch 155 of 202\n",
      "Added batch 156 of 202\n",
      "Added batch 157 of 202\n",
      "Added batch 158 of 202\n",
      "Added batch 159 of 202\n",
      "Added batch 160 of 202\n",
      "Added batch 161 of 202\n",
      "Added batch 162 of 202\n",
      "Added batch 163 of 202\n",
      "Added batch 164 of 202\n",
      "Added batch 165 of 202\n",
      "Added batch 166 of 202\n",
      "Added batch 167 of 202\n",
      "Added batch 168 of 202\n",
      "Added batch 169 of 202\n",
      "Added batch 170 of 202\n",
      "Added batch 171 of 202\n",
      "Added batch 172 of 202\n",
      "Added batch 173 of 202\n",
      "Added batch 174 of 202\n",
      "Added batch 175 of 202\n",
      "Added batch 176 of 202\n",
      "Added batch 177 of 202\n",
      "Added batch 178 of 202\n",
      "Added batch 179 of 202\n",
      "Added batch 180 of 202\n",
      "Added batch 181 of 202\n",
      "Added batch 182 of 202\n",
      "Added batch 183 of 202\n",
      "Added batch 184 of 202\n",
      "Added batch 185 of 202\n",
      "Added batch 186 of 202\n",
      "Added batch 187 of 202\n",
      "Added batch 188 of 202\n",
      "Added batch 189 of 202\n",
      "Added batch 190 of 202\n",
      "Added batch 191 of 202\n",
      "Added batch 192 of 202\n",
      "Added batch 193 of 202\n",
      "Added batch 194 of 202\n",
      "Added batch 195 of 202\n",
      "Added batch 196 of 202\n",
      "Added batch 197 of 202\n",
      "Added batch 198 of 202\n",
      "Added batch 199 of 202\n",
      "Added batch 200 of 202\n",
      "Added batch 201 of 202\n",
      "Added batch 202 of 202\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define the file path\n",
    "input_path = \"fuzzified56k(200k).txt\"\n",
    "\n",
    "# Read names from file\n",
    "with open(input_path, \"r\") as file:\n",
    "    names = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Function to vectorize name parts\n",
    "def vectorize_name_parts(name):\n",
    "    # Remove special characters\n",
    "    cleaned_name = re.sub(r'[^\\w\\s]|[\\d]', '', name)\n",
    "    parts = [part for part in cleaned_name.split() if len(part) > 2]\n",
    "    ipa_parts = [english_to_ipa(part) for part in parts]\n",
    "    vectors = [ipa2vec(ipa) for ipa in ipa_parts]\n",
    "    return parts, ipa_parts, vectors\n",
    "\n",
    "# Function to generate a random Aadhaar card number\n",
    "def generate_aadhaar():\n",
    "    return ''.join([str(random.randint(0, 9)) for _ in range(12)])\n",
    "\n",
    "# Function to generate a random date of birth and date of crime\n",
    "def generate_dob_and_doc():\n",
    "    start_date_dob = datetime.strptime('1950-01-01', '%Y-%m-%d')\n",
    "    end_date_dob = datetime.strptime('2005-12-31', '%Y-%m-%d')\n",
    "    random_date_dob = start_date_dob + timedelta(days=random.randint(0, (end_date_dob - start_date_dob).days))\n",
    "    dob = random_date_dob.strftime('%d-%m-%Y')\n",
    "    age = datetime.now().year - random_date_dob.year\n",
    "\n",
    "    end_date_doc = datetime.now()\n",
    "    start_date_doc = min(random_date_dob + timedelta(days=365 * 20), end_date_doc)  # Ensure doc is at least 20 years after dob\n",
    "    if start_date_doc < end_date_doc:\n",
    "        random_date_doc = start_date_doc + timedelta(days=random.randint(0, (end_date_doc - start_date_doc).days))\n",
    "    else:\n",
    "        random_date_doc = start_date_doc\n",
    "    doc = random_date_doc.strftime('%d-%m-%Y')\n",
    "\n",
    "    return dob, age, doc\n",
    "\n",
    "# Calculate the number of batches\n",
    "batch_size = 1000  # Choose a batch size less than the maximum\n",
    "num_batches = math.ceil(len(names) / batch_size)\n",
    "\n",
    "# Add data in batches\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(names))\n",
    "\n",
    "    batch_names = names[start_idx:end_idx]\n",
    "    batch_embeddings = []\n",
    "    batch_metadatas = []\n",
    "    batch_ids = []\n",
    "    for name_idx, name in enumerate(batch_names):\n",
    "        # Append a random surname\n",
    "        surname = random.choice(common_indian_surnames)\n",
    "        full_name = f\"{name} {surname}\"\n",
    "        parts, ipa_parts, vectors = vectorize_name_parts(full_name)\n",
    "        if vectors:\n",
    "            type = random.choice([\"Criminal\", \"Suspect\", \"Victim\", \"Witness\"])\n",
    "            aadhaar = generate_aadhaar()\n",
    "            dob, age, doc = generate_dob_and_doc()\n",
    "            gender = 'Male'  # Randomly assign gender\n",
    "            police_station = random.choice(police_stations)  # Randomly select a police station\n",
    "            crime = random.choice(ipc_crimes)  # Randomly select a crime\n",
    "            address = random.choice(addresses)  # Randomly select an address\n",
    "            for part_idx, (part, ipa, vector) in enumerate(zip(parts, ipa_parts, vectors)):\n",
    "                if vector:\n",
    "                    batch_embeddings.append(vector)\n",
    "                    batch_metadatas.append({\"full_name\": full_name, \"part\": part, \"ipa\": ipa, \"age\": age, \"aadhaar\": aadhaar, \"gender\": gender, \"dob\": dob, \"doc\": doc, \"station\": police_station, \"crime\": crime, \"address\": address, \"type\": type,\"trans_name\":trans_name})\n",
    "                    batch_ids.append(f\"{start_idx + name_idx}_{part_idx}\")\n",
    "                else:\n",
    "                    reason = \"is nan\" \n",
    "                    print(f\"Skipping vector for part '{part}' in name '{full_name}'({reason}) due to invalid vector: {vector} \")\n",
    "    # print(f\"Batch {i + 1} embeddings sample: {batch_embeddings[:4]}\")\n",
    "    if batch_embeddings:  # Ensure there is data to add\n",
    "        collection.add(\n",
    "            documents=[metadata[\"full_name\"] for metadata in batch_metadatas],\n",
    "            embeddings=batch_embeddings,\n",
    "            ids=batch_ids,\n",
    "            metadatas=batch_metadatas\n",
    "        )\n",
    "        print(f\"Added batch {i + 1} of {num_batches}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcgXQRoWHHVH"
   },
   "source": [
    "chroma won't allow u to add >40k entries in a single add request, so u have to do it in batches and also think about fixing the unique id problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For hindi singles:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping vector for name ''(is nan) due to invalid vector: []\n",
      "Added batch 1 of 21\n",
      "Added batch 2 of 21\n",
      "Added batch 3 of 21\n",
      "Added batch 4 of 21\n",
      "Added batch 5 of 21\n",
      "Added batch 6 of 21\n",
      "Added batch 7 of 21\n",
      "Added batch 8 of 21\n",
      "Added batch 9 of 21\n",
      "Added batch 10 of 21\n",
      "Added batch 11 of 21\n",
      "Skipping vector for name ''(is nan) due to invalid vector: []\n",
      "Skipping vector for name ''(is nan) due to invalid vector: []\n",
      "Skipping vector for name ''(is nan) due to invalid vector: []\n",
      "Skipping vector for name ''(is nan) due to invalid vector: []\n",
      "Skipping vector for name ''(is nan) due to invalid vector: []\n",
      "Added batch 12 of 21\n",
      "Skipping vector for name ''(is nan) due to invalid vector: []\n",
      "Skipping vector for name ''(is nan) due to invalid vector: []\n",
      "Skipping vector for name ''(is nan) due to invalid vector: []\n",
      "Skipping vector for name ''(is nan) due to invalid vector: []\n",
      "Added batch 13 of 21\n",
      "Added batch 14 of 21\n",
      "Added batch 15 of 21\n",
      "Added batch 16 of 21\n",
      "Added batch 17 of 21\n",
      "Added batch 18 of 21\n",
      "Added batch 19 of 21\n",
      "Added batch 20 of 21\n",
      "Added batch 21 of 21\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define the file path\n",
    "input_path = \"fuzzified56k(200k).txt\"\n",
    "\n",
    "# Read names from file\n",
    "with open(input_path, \"r\") as file:\n",
    "    names = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Read Hindi names from file\n",
    "hindi_input_path = \"hindiv3.txt\"\n",
    "with open(hindi_input_path, \"r\") as file:\n",
    "    hindi_names = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Function to vectorize a single-word name\n",
    "def vectorize_single_word_name(name):\n",
    "    # Remove special characters\n",
    "    cleaned_name = re.sub(r'[^\\w\\s]|[\\d]', '', name)\n",
    "    ipa = english_to_ipa(cleaned_name)\n",
    "    vector = ipa2vec(ipa)\n",
    "    return cleaned_name, ipa, vector\n",
    "\n",
    "# Function to generate a random Aadhaar card number\n",
    "def generate_aadhaar():\n",
    "    return ''.join([str(random.randint(0, 9)) for _ in range(12)])\n",
    "\n",
    "# Function to generate a random date of birth and date of crime\n",
    "def generate_dob_and_doc():\n",
    "    start_date_dob = datetime.strptime('1950-01-01', '%Y-%m-%d')\n",
    "    end_date_dob = datetime.strptime('2005-12-31', '%Y-%m-%d')\n",
    "    random_date_dob = start_date_dob + timedelta(days=random.randint(0, (end_date_dob - start_date_dob).days))\n",
    "    dob = random_date_dob.strftime('%d-%m-%Y')\n",
    "    age = datetime.now().year - random_date_dob.year\n",
    "\n",
    "    end_date_doc = datetime.now()\n",
    "    start_date_doc = min(random_date_dob + timedelta(days=365 * 20), end_date_doc)  # Ensure doc is at least 20 years after dob\n",
    "    if start_date_doc < end_date_doc:\n",
    "        random_date_doc = start_date_doc + timedelta(days=random.randint(0, (end_date_doc - start_date_doc).days))\n",
    "    else:\n",
    "        random_date_doc = start_date_doc\n",
    "    doc = random_date_doc.strftime('%d-%m-%Y')\n",
    "\n",
    "    return dob, age, doc\n",
    "\n",
    "# Calculate the number of batches\n",
    "batch_size = 10000  # Choose a batch size less than the maximum\n",
    "num_batches = math.ceil(len(names) / batch_size)\n",
    "\n",
    "# Add data in batches\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(names))\n",
    "\n",
    "    batch_names = names[start_idx:end_idx]\n",
    "    batch_hindi_names = hindi_names[start_idx:end_idx]\n",
    "    batch_embeddings = []\n",
    "    batch_metadatas = []\n",
    "    batch_ids = []\n",
    "    for name_idx, (name, hindi_name) in enumerate(zip(batch_names, batch_hindi_names)):\n",
    "        cleaned_name, ipa, vector = vectorize_single_word_name(name)\n",
    "        if vector:\n",
    "            type = random.choice([\"Criminal\", \"Suspect\", \"Victim\", \"Witness\"])\n",
    "            aadhaar = generate_aadhaar()\n",
    "            dob, age, doc = generate_dob_and_doc()\n",
    "            gender = 'Male'  # Randomly assign gender\n",
    "            police_station = random.choice(police_stations)  # Randomly select a police station\n",
    "            crime = random.choice(ipc_crimes)  # Randomly select a crime\n",
    "            address = random.choice(addresses)  # Randomly select an address\n",
    "            batch_embeddings.append(vector)\n",
    "            batch_metadatas.append({\"full_name\": cleaned_name, \"ipa\": ipa, \"age\": age, \"aadhaar\": aadhaar, \"gender\": gender, \"dob\": dob, \"doc\": doc, \"station\": police_station, \"crime\": crime, \"address\": address, \"type\": type, \"trans_name\": hindi_name})\n",
    "            batch_ids.append(f\"{start_idx + name_idx}\")\n",
    "        else:\n",
    "            reason = \"is nan\" \n",
    "            print(f\"Skipping vector for name '{cleaned_name}'({reason}) due to invalid vector: {vector}\")\n",
    "    # print(f\"Batch {i + 1} embeddings sample: {batch_embeddings[:4]}\")\n",
    "    if batch_embeddings:  # Ensure there is data to add\n",
    "        collection.add(\n",
    "            documents=[metadata[\"full_name\"] for metadata in batch_metadatas],\n",
    "            embeddings=batch_embeddings,\n",
    "            ids=batch_ids,\n",
    "            metadatas=batch_metadatas\n",
    "        )\n",
    "        print(f\"Added batch {i + 1} of {num_batches}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single name only:- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XidfgaI_B_t0",
    "outputId": "74a158f7-0358-4626-c7f6-925d9c34b805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added batch 1 of 21\n",
      "Added batch 2 of 21\n",
      "Added batch 3 of 21\n",
      "Added batch 4 of 21\n",
      "Added batch 5 of 21\n",
      "Added batch 6 of 21\n",
      "Added batch 7 of 21\n",
      "Added batch 8 of 21\n",
      "Added batch 9 of 21\n",
      "Added batch 10 of 21\n",
      "Added batch 11 of 21\n",
      "Added batch 12 of 21\n",
      "Added batch 13 of 21\n",
      "Added batch 14 of 21\n",
      "Added batch 15 of 21\n",
      "Added batch 16 of 21\n",
      "Added batch 17 of 21\n",
      "Added batch 18 of 21\n",
      "Added batch 19 of 21\n",
      "Added batch 20 of 21\n",
      "Added batch 21 of 21\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import re\n",
    "import numpy as npz\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define the file path\n",
    "input_path = \"fuzzified56k(200k).txt\"\n",
    "\n",
    "# Read names from file\n",
    "with open(input_path, \"r\") as file:\n",
    "    names = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Function to vectorize name parts\n",
    "def vectorize_name_parts(name):\n",
    "    # Remove special characters\n",
    "    cleaned_name = re.sub(r'[^\\w\\s]|[\\d]', '', name)\n",
    "    parts = [part for part in cleaned_name.split() if len(part) > 2]\n",
    "    ipa_parts = [english_to_ipa(part) for part in parts]\n",
    "    vectors = [ipa2vec(ipa) for ipa in ipa_parts]\n",
    "    return parts, ipa_parts, vectors\n",
    "\n",
    "# Function to generate a random Aadhaar card number\n",
    "def generate_aadhaar():\n",
    "    return ''.join([str(random.randint(0, 9)) for _ in range(12)])\n",
    "\n",
    "# Function to generate a random date of birth and date of crime\n",
    "def generate_dob_and_doc():\n",
    "    start_date_dob = datetime.strptime('1950-01-01', '%Y-%m-%d')\n",
    "    end_date_dob = datetime.strptime('2005-12-31', '%Y-%m-%d')\n",
    "    random_date_dob = start_date_dob + timedelta(days=random.randint(0, (end_date_dob - start_date_dob).days))\n",
    "    dob = random_date_dob.strftime('%d-%m-%Y')\n",
    "    age = datetime.now().year - random_date_dob.year\n",
    "\n",
    "    end_date_doc = datetime.now()\n",
    "    start_date_doc = min(random_date_dob + timedelta(days=365 * 20), end_date_doc)  # Ensure doc is at least 20 years after dob\n",
    "    if start_date_doc < end_date_doc:\n",
    "        random_date_doc = start_date_doc + timedelta(days=random.randint(0, (end_date_doc - start_date_doc).days))\n",
    "    else:\n",
    "        random_date_doc = start_date_doc\n",
    "    doc = random_date_doc.strftime('%d-%m-%Y')\n",
    "\n",
    "    return dob, age, doc\n",
    "\n",
    "\n",
    "# Set to keep track of unique parts\n",
    "unique_parts = set()\n",
    "# Calculate the number of batches\n",
    "batch_size = 10000  # Choose a batch size less than the maximum\n",
    "num_batches = math.ceil(len(names) / batch_size)\n",
    "\n",
    "# Add data in batches\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(names))\n",
    "\n",
    "    batch_names = names[start_idx:end_idx]\n",
    "    batch_embeddings = []\n",
    "    batch_metadatas = []\n",
    "    batch_ids = []\n",
    "    for name_idx, name in enumerate(batch_names):\n",
    "        parts, ipa_parts, vectors = vectorize_name_parts(name)\n",
    "        for part_idx, (part, ipa, vector) in enumerate(zip(parts, ipa_parts, vectors)):\n",
    "            part = part.title()\n",
    "            if part not in unique_parts:\n",
    "                unique_parts.add(part)\n",
    "                aadhaar = generate_aadhaar()\n",
    "                dob, age, doc = generate_dob_and_doc()\n",
    "                gender = 'Male'  # Randomly assign gender\n",
    "                police_station = random.choice(police_stations)  # Randomly select a police station\n",
    "                crime = random.choice(ipc_crimes)  # Randomly select a crime\n",
    "                address = random.choice(addresses)  # Randomly select an address\n",
    "                batch_embeddings.append(vector)\n",
    "                batch_metadatas.append({\"full_name\": name, \"part\": part, \"ipa\": ipa, \"age\": age, \"aadhaar\": aadhaar, \"gender\": gender, \"dob\": dob, \"doc\": doc, \"station\": police_station, \"crime\": crime, \"address\": address})\n",
    "                batch_ids.append(f\"{start_idx + name_idx}_{part_idx}\")\n",
    "\n",
    "    # print(f\"Batch {i + 1} embeddings sample: {batch_embeddings[:4]}\")\n",
    "    if batch_embeddings:  # Ensure there is data to add\n",
    "        collection.add(\n",
    "            documents=[metadata[\"part\"] for metadata in batch_metadatas],\n",
    "            embeddings=batch_embeddings,\n",
    "            ids=batch_ids,\n",
    "            metadatas=batch_metadatas\n",
    "        )\n",
    "        print(f\"Added batch {i + 1} of {num_batches}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8hpq-ZXBmKe"
   },
   "source": [
    "## Else:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NfBiFrU-MJ1"
   },
   "outputs": [],
   "source": [
    "# Add pronunciations to the ChromaDB collection\n",
    "embedder(names, ipas)\n",
    "\n",
    "# all_embeddings = collection.get(include=[\"embeddings\"])[\"embeddings\"]\n",
    "# print(\"All embeddings:\", all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtKjmqtMxhrX"
   },
   "source": [
    "### To input hindi data :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ww4UvhaVw-Oi"
   },
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "input_path = \"sname.txt\"\n",
    "# Read names from file\n",
    "with open(input_path, \"r\") as file:\n",
    "    names = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Convert names to IPA\n",
    "ipas = [hindi_to_ipa(name) for name in names]\n",
    "\n",
    "# Add pronunciations to the ChromaDB collection\n",
    "embedder(names, ipas)\n",
    "\n",
    "all_embeddings = collection.get(include=[\"embeddings\"])[\"embeddings\"]\n",
    "# print(\"All embeddings:\", all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9im3UaA995-"
   },
   "source": [
    "# Querry:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69jPXYVQ95no",
    "outputId": "f92104b2-01bb-486f-96bd-8a75675415e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Laxmi, ChromaDB Distance: 0.0, Combined Score: 0.0, Weighted Edit Distance: 0.0, ID: 19602\n",
      "Document: Laxmee, ChromaDB Distance: 0.0, Combined Score: 0.0, Weighted Edit Distance: 0.0, ID: 48158\n",
      "Document: Laxmii, ChromaDB Distance: 0.0, Combined Score: 0.0, Weighted Edit Distance: 0.0, ID: 48233\n",
      "Document: Laxhmi, ChromaDB Distance: 0.0, Combined Score: 0.0, Weighted Edit Distance: 0.0, ID: 48230\n",
      "Document: Lakhsmi, ChromaDB Distance: 0.0, Combined Score: 0.0, Weighted Edit Distance: 0.0, ID: 48281\n",
      "Document: Laxmmi, ChromaDB Distance: 0.0, Combined Score: 0.0, Weighted Edit Distance: 0.0, ID: 48244\n",
      "Document: Laxmy, ChromaDB Distance: 0.0, Combined Score: 0.0, Weighted Edit Distance: 0.0, ID: 48239\n",
      "Document: Laksmi, ChromaDB Distance: 0.0, Combined Score: 0.0, Weighted Edit Distance: 0.0, ID: 154969\n",
      "Document: Lakhshmi, ChromaDB Distance: 0.0, Combined Score: 0.0, Weighted Edit Distance: 0.0, ID: 155820\n",
      "Document: laxmi, ChromaDB Distance: 0.0, Combined Score: 0.0, Weighted Edit Distance: 0.0, ID: 194447\n",
      "Document: Lakhsmee, ChromaDB Distance: 0.005447089672088623, Combined Score: 0.07881296277046203, Weighted Edit Distance: 0.25, ID: 48229\n",
      "Document: Lakzmi, ChromaDB Distance: 0.005447089672088623, Combined Score: 0.07881296277046203, Weighted Edit Distance: 0.25, ID: 154970\n",
      "Document: Laaxmee, ChromaDB Distance: 0.0067754387855529785, Combined Score: 0.22974280714988707, Weighted Edit Distance: 0.75, ID: 156362\n",
      "Document: Luxmee, ChromaDB Distance: 0.011173605918884277, Combined Score: 0.307821524143219, Weighted Edit Distance: 1.0, ID: 48949\n",
      "Document: Luxmii, ChromaDB Distance: 0.011173605918884277, Combined Score: 0.307821524143219, Weighted Edit Distance: 1.0, ID: 48948\n",
      "Document: Luxmi, ChromaDB Distance: 0.011173605918884277, Combined Score: 0.307821524143219, Weighted Edit Distance: 1.0, ID: 50547\n",
      "Document: Luxmy, ChromaDB Distance: 0.011173605918884277, Combined Score: 0.307821524143219, Weighted Edit Distance: 1.0, ID: 50548\n",
      "Document: Luckssmi, ChromaDB Distance: 0.011173605918884277, Combined Score: 0.307821524143219, Weighted Edit Distance: 1.0, ID: 157327\n",
      "Document: Luksmi, ChromaDB Distance: 0.011173605918884277, Combined Score: 0.307821524143219, Weighted Edit Distance: 1.0, ID: 157352\n",
      "Document: Lucksmi, ChromaDB Distance: 0.011173605918884277, Combined Score: 0.307821524143219, Weighted Edit Distance: 1.0, ID: 157324\n",
      "Document: Luxmye, ChromaDB Distance: 0.011173605918884277, Combined Score: 0.307821524143219, Weighted Edit Distance: 1.0, ID: 157422\n",
      "Document: Luxmie, ChromaDB Distance: 0.011173605918884277, Combined Score: 0.307821524143219, Weighted Edit Distance: 1.0, ID: 157421\n",
      "Document: Laqshmi, ChromaDB Distance: 0.01142895221710205, Combined Score: 0.23300026655197142, Weighted Edit Distance: 0.75, ID: 48157\n",
      "Document: Lakshmi, ChromaDB Distance: 0.01142895221710205, Combined Score: 0.23300026655197142, Weighted Edit Distance: 0.75, ID: 48156\n",
      "Document: Lakshmee, ChromaDB Distance: 0.01142895221710205, Combined Score: 0.23300026655197142, Weighted Edit Distance: 0.75, ID: 48172\n",
      "Document: Lakshmy, ChromaDB Distance: 0.01142895221710205, Combined Score: 0.23300026655197142, Weighted Edit Distance: 0.75, ID: 154954\n",
      "Document: Lakshmii, ChromaDB Distance: 0.01142895221710205, Combined Score: 0.23300026655197142, Weighted Edit Distance: 0.75, ID: 155878\n",
      "Document: Lakhshnee, ChromaDB Distance: 0.011999189853668213, Combined Score: 0.3458994328975677, Weighted Edit Distance: 1.125, ID: 155859\n",
      "Document: Laxhnee, ChromaDB Distance: 0.011999189853668213, Combined Score: 0.3458994328975677, Weighted Edit Distance: 1.125, ID: 155840\n",
      "Document: Laxhni, ChromaDB Distance: 0.011999189853668213, Combined Score: 0.3458994328975677, Weighted Edit Distance: 1.125, ID: 155817\n",
      "['Laxmi', 'Laxmee', 'Laxmii', 'Laxhmi', 'Lakhsmi', 'Laxmmi', 'Laxmy', 'Laksmi', 'Lakhshmi', 'laxmi', 'Lakhsmee', 'Lakzmi', 'Laaxmee', 'Luxmee', 'Luxmii', 'Luxmi', 'Luxmy', 'Luckssmi', 'Luksmi', 'Lucksmi', 'Luxmye', 'Luxmie', 'Laqshmi', 'Lakshmi', 'Lakshmee', 'Lakshmy', 'Lakshmii', 'Lakhshnee', 'Laxhnee', 'Laxhni']\n",
      "[{'aadhaar': '528592499713', 'address': 'Plot No. 65, Shankar Nagar, Morena, Madhya Pradesh - 476001', 'age': 46, 'crime': 'Rape - IPC Section 376', 'dob': '05-11-1978', 'doc': '09-11-2000', 'full_name': 'Laxmi', 'gender': 'Male', 'ipa': 'læksmi', 'station': 'Sirmour', 'trans_name': 'लक्ष्मी', 'type': 'Witness'}, {'aadhaar': '925026822224', 'address': 'Flat No. 401, Emerald Heights, Indore, Madhya Pradesh - 452010', 'age': 24, 'crime': 'Indecent Representation of Women - IPC Section 66E', 'dob': '22-03-2000', 'doc': '19-09-2022', 'full_name': 'Laxmee', 'gender': 'Male', 'ipa': 'læksmi', 'station': 'Jaisinagar', 'trans_name': 'लक्ष्मी', 'type': 'Witness'}, {'aadhaar': '891637035407', 'address': 'Flat No. 202, Sai Residency, Hoshangabad, Madhya Pradesh - 461001', 'age': 57, 'crime': 'Forgery - IPC Section 463', 'dob': '10-02-1967', 'doc': '25-08-2013', 'full_name': 'Laxmii', 'gender': 'Male', 'ipa': 'læksmi', 'station': 'Kanadiya', 'trans_name': 'लक्ष्मी', 'type': 'Witness'}, {'aadhaar': '014750336507', 'address': '78, Station Road, Jabalpur, Madhya Pradesh - 482001', 'age': 41, 'crime': 'Arson - IPC Section 436', 'dob': '16-04-1983', 'doc': '27-08-2004', 'full_name': 'Laxhmi', 'gender': 'Male', 'ipa': 'læksmi', 'station': 'Rehli', 'trans_name': 'लक्ष्मी', 'type': 'Suspect'}, {'aadhaar': '590336308559', 'address': 'Flat No. 102, Galaxy Heights, Bhind, Madhya Pradesh - 477001', 'age': 58, 'crime': 'Rioting - IPC Section 147', 'dob': '02-07-1966', 'doc': '17-03-2009', 'full_name': 'Lakhsmi', 'gender': 'Male', 'ipa': 'læksmi', 'station': 'Hatod', 'trans_name': 'लक्ष्मी', 'type': 'Suspect'}, {'aadhaar': '112746642515', 'address': 'Plot No. 90, Navjeevan Colony, Dewas, Madhya Pradesh - 455001', 'age': 46, 'crime': 'Causing Grievous Hurt - IPC Section 320', 'dob': '09-12-1978', 'doc': '19-04-2022', 'full_name': 'Laxmmi', 'gender': 'Male', 'ipa': 'læksmi', 'station': 'Binaika', 'trans_name': 'लक्ष्मी', 'type': 'Suspect'}, {'aadhaar': '071565711198', 'address': 'House No. 110, Saraswati Colony, Shahdol, Madhya Pradesh - 484001', 'age': 30, 'crime': 'Terrorism - IPC Section 121', 'dob': '05-03-1994', 'doc': '07-01-2017', 'full_name': 'Laxmy', 'gender': 'Male', 'ipa': 'læksmi', 'station': 'Banda', 'trans_name': 'लक्ष्मी', 'type': 'Witness'}, {'aadhaar': '368315661916', 'address': 'House No. 78, Ram Nagar, Tikamgarh, Madhya Pradesh - 472001', 'age': 60, 'crime': 'Abetment of Suicide - IPC Section 306', 'dob': '11-02-1964', 'doc': '02-01-2007', 'full_name': 'Laksmi', 'gender': 'Male', 'ipa': 'læksmi', 'station': 'SDOP Rehli', 'trans_name': 'लक्ष्मी', 'type': 'Witness'}, {'aadhaar': '506173249480', 'address': 'Plot No. 89, Shanti Nagar, Rewa, Madhya Pradesh - 486001', 'age': 72, 'crime': 'Rape - IPC Section 376', 'dob': '01-08-1952', 'doc': '12-11-1997', 'full_name': 'Lakhshmi', 'gender': 'Male', 'ipa': 'læksmi', 'station': 'Sanwer', 'trans_name': 'लक्ष्मी', 'type': 'Suspect'}, {'aadhaar': '548819314432', 'address': 'H.No. 304, Shanti Vihar, Neemuch, Madhya Pradesh - 458441', 'age': 35, 'crime': 'Arson - IPC Section 436', 'dob': '22-06-1989', 'doc': '08-06-2012', 'full_name': 'laxmi', 'gender': 'Male', 'ipa': 'læksmi', 'station': 'Mhow', 'trans_name': 'लक्ष्मी', 'type': 'Suspect'}, {'aadhaar': '005942528596', 'address': 'Plot No. 65, Shankar Nagar, Morena, Madhya Pradesh - 476001', 'age': 50, 'crime': 'Terrorism - IPC Section 121', 'dob': '16-01-1974', 'doc': '05-03-2003', 'full_name': 'Lakhsmee', 'gender': 'Male', 'ipa': 'lækzmi', 'station': 'Sadar Bazar', 'trans_name': 'लक्ष्मी', 'type': 'Criminal'}, {'aadhaar': '796099670498', 'address': 'House No. 67, Kamla Nagar, Shivpuri, Madhya Pradesh - 473551', 'age': 49, 'crime': 'Rape - IPC Section 376', 'dob': '23-01-1975', 'doc': '22-03-2023', 'full_name': 'Lakzmi', 'gender': 'Male', 'ipa': 'lækzmi', 'station': 'Dabhora', 'trans_name': 'लक्ष्मी', 'type': 'Victim'}, {'aadhaar': '343414242667', 'address': 'Plot No. 120, Malviya Nagar, Sehore, Madhya Pradesh - 466001', 'age': 20, 'crime': 'Unlawful Assembly - IPC Section 141', 'dob': '11-12-2004', 'doc': '09-12-2024', 'full_name': 'Laaxmee', 'gender': 'Male', 'ipa': 'lɑksmi', 'station': 'Khajrana', 'trans_name': 'लाक्समी', 'type': 'Victim'}, {'aadhaar': '056527153036', 'address': 'H.No. 55, Jai Stambh Chowk, Singrauli, Madhya Pradesh - 486889', 'age': 23, 'crime': 'Murder - IPC Section 302', 'dob': '20-11-2001', 'doc': '07-09-2023', 'full_name': 'Luxmee', 'gender': 'Male', 'ipa': 'lʌksmi', 'station': 'Kotwali', 'trans_name': 'लक्समी', 'type': 'Witness'}, {'aadhaar': '872438882402', 'address': 'Plot No. 35, Narmada Colony, Khandwa, Madhya Pradesh - 450001', 'age': 51, 'crime': 'Murder - IPC Section 302', 'dob': '11-04-1973', 'doc': '15-05-1996', 'full_name': 'Luxmii', 'gender': 'Male', 'ipa': 'lʌksmi', 'station': 'Kishanganj', 'trans_name': 'लक्समी', 'type': 'Criminal'}, {'aadhaar': '617927241933', 'address': 'House No. 128, Vijay Nagar, Gwalior, Madhya Pradesh - 474011', 'age': 48, 'crime': 'Embezzlement - IPC Section 403', 'dob': '22-02-1976', 'doc': '22-10-2007', 'full_name': 'Luxmi', 'gender': 'Male', 'ipa': 'lʌksmi', 'station': 'Tilak Nagar', 'trans_name': 'लक्ष्मी', 'type': 'Suspect'}, {'aadhaar': '556254838762', 'address': '123, Gandhi Nagar, Khargone, Madhya Pradesh - 451001', 'age': 70, 'crime': 'Rioting - IPC Section 147', 'dob': '29-10-1954', 'doc': '29-11-1993', 'full_name': 'Luxmy', 'gender': 'Male', 'ipa': 'lʌksmi', 'station': 'Khudail', 'trans_name': 'लक्समी', 'type': 'Criminal'}, {'aadhaar': '151275730155', 'address': 'Plot No. 65, Shankar Nagar, Morena, Madhya Pradesh - 476001', 'age': 48, 'crime': 'Terrorism - IPC Section 121', 'dob': '27-10-1976', 'doc': '22-01-1999', 'full_name': 'Luckssmi', 'gender': 'Male', 'ipa': 'lʌksmi', 'station': 'Jaisinagar', 'trans_name': 'लक्सस्मी', 'type': 'Witness'}, {'aadhaar': '285341433453', 'address': 'Plot No. 14, Sector C, Govindpura, Bhopal, Madhya Pradesh - 462023', 'age': 38, 'crime': 'Dacoity - IPC Section 395', 'dob': '07-06-1986', 'doc': '23-08-2023', 'full_name': 'Luksmi', 'gender': 'Male', 'ipa': 'lʌksmi', 'station': 'Banda', 'trans_name': 'लक्समी', 'type': 'Suspect'}, {'aadhaar': '398619804608', 'address': 'House No. 22, Shastri Colony, Betul, Madhya Pradesh - 460001', 'age': 26, 'crime': 'Kidnapping - IPC Section 361', 'dob': '23-11-1998', 'doc': '06-07-2020', 'full_name': 'Lucksmi', 'gender': 'Male', 'ipa': 'lʌksmi', 'station': 'Shahpur', 'trans_name': 'लकस्मी', 'type': 'Witness'}, {'aadhaar': '856790304843', 'address': 'Shop No. 10, Bus Stand Road, Chhindwara, Madhya Pradesh - 480001', 'age': 66, 'crime': 'Criminal Breach of Trust - IPC Section 406', 'dob': '29-05-1958', 'doc': '03-02-2020', 'full_name': 'Luxmye', 'gender': 'Male', 'ipa': 'lʌksmi', 'station': 'Pandrinath', 'trans_name': 'लक्समी', 'type': 'Victim'}, {'aadhaar': '680293681033', 'address': '78, Station Road, Jabalpur, Madhya Pradesh - 482001', 'age': 65, 'crime': 'Forgery - IPC Section 463', 'dob': '27-02-1959', 'doc': '19-09-2014', 'full_name': 'Luxmie', 'gender': 'Male', 'ipa': 'lʌksmi', 'station': 'Antrela', 'trans_name': 'लक्समी', 'type': 'Witness'}, {'aadhaar': '222904157471', 'address': 'Shop No. 10, Bus Stand Road, Chhindwara, Madhya Pradesh - 480001', 'age': 59, 'crime': 'Bigamy - IPC Section 494', 'dob': '19-07-1965', 'doc': '22-04-1990', 'full_name': 'Laqshmi', 'gender': 'Male', 'ipa': 'lækʃmi', 'station': 'Baheria', 'trans_name': 'लक्ष्मी', 'type': 'Victim'}, {'aadhaar': '998072383903', 'address': 'House No. 78, Ram Nagar, Tikamgarh, Madhya Pradesh - 472001', 'age': 32, 'crime': 'Trespass - IPC Section 441', 'dob': '01-04-1992', 'doc': '29-08-2012', 'full_name': 'Lakshmi', 'gender': 'Male', 'ipa': 'lækʃmi', 'station': 'Lasudia', 'trans_name': 'लक्ष्मी', 'type': 'Criminal'}, {'aadhaar': '205588696784', 'address': 'House No. 22, Shastri Colony, Betul, Madhya Pradesh - 460001', 'age': 38, 'crime': 'Criminal Intimidation - IPC Section 506', 'dob': '18-12-1986', 'doc': '03-10-2012', 'full_name': 'Lakshmee', 'gender': 'Male', 'ipa': 'lækʃmi', 'station': 'Vijay Nagar', 'trans_name': 'लक्ष्मी', 'type': 'Suspect'}, {'aadhaar': '563950509476', 'address': 'Shop No. 12, Main Bazaar, Satna, Madhya Pradesh - 485001', 'age': 64, 'crime': 'Abetment of Suicide - IPC Section 306', 'dob': '22-01-1960', 'doc': '16-04-1991', 'full_name': 'Lakshmy', 'gender': 'Male', 'ipa': 'lækʃmi', 'station': 'Chhanbila', 'trans_name': 'लक्ष्मी', 'type': 'Witness'}, {'aadhaar': '444179115328', 'address': 'Shop No. 3, Main Road, Panna, Madhya Pradesh - 488001', 'age': 31, 'crime': 'Waging War Against the Government - IPC Section 121', 'dob': '07-08-1993', 'doc': '22-04-2023', 'full_name': 'Lakshmii', 'gender': 'Male', 'ipa': 'lækʃmi', 'station': 'Naigarhi', 'trans_name': 'लक्ष्मी', 'type': 'Witness'}, {'aadhaar': '470830982003', 'address': 'Shop No. 11, Rajwada, Datia, Madhya Pradesh - 475661', 'age': 26, 'crime': 'Criminal Intimidation - IPC Section 506', 'dob': '26-03-1998', 'doc': '21-03-2024', 'full_name': 'Lakhshnee', 'gender': 'Male', 'ipa': 'læksni', 'station': 'Dalpatpur', 'trans_name': 'लाखनी', 'type': 'Suspect'}, {'aadhaar': '849546775869', 'address': 'Shop No. 21, Market Road, Umaria, Madhya Pradesh - 484661', 'age': 27, 'crime': 'Forgery - IPC Section 463', 'dob': '04-08-1997', 'doc': '01-11-2017', 'full_name': 'Laxhnee', 'gender': 'Male', 'ipa': 'læksni', 'station': 'Pardeshipura', 'trans_name': 'लक्ष्नी', 'type': 'Suspect'}, {'aadhaar': '694604461829', 'address': 'House No. 45, Sindhi Colony, Sagar, Madhya Pradesh - 470001', 'age': 51, 'crime': 'Rioting - IPC Section 147', 'dob': '30-03-1973', 'doc': '12-05-2002', 'full_name': 'Laxhni', 'gender': 'Male', 'ipa': 'læksni', 'station': 'Khudail', 'trans_name': 'लक्ष्मी', 'type': 'Suspect'}]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005447089672088623, 0.005447089672088623, 0.0067754387855529785, 0.011173605918884277, 0.011173605918884277, 0.011173605918884277, 0.011173605918884277, 0.011173605918884277, 0.011173605918884277, 0.011173605918884277, 0.011173605918884277, 0.011173605918884277, 0.01142895221710205, 0.01142895221710205, 0.01142895221710205, 0.01142895221710205, 0.01142895221710205, 0.011999189853668213, 0.011999189853668213, 0.011999189853668213]\n"
     ]
    }
   ],
   "source": [
    "query_name = \"laxmi\"\n",
    "results = querrier(query_name)\n",
    "print(results['documents'])\n",
    "print(results['metadatas'])\n",
    "print(results['distances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IPA representation of 'Aarav' is: æɹɑv\n",
      "Document: Aaravi, ChromaDB Distance: -1.1920928955078125e-07, Combined Score: -8.344650268554687e-08, Weighted Edit Distance: 0.0, ID: 116421_0\n",
      "Document: Aarabi, ChromaDB Distance: 0.013513386249542236, Combined Score: 0.3844593703746796, Weighted Edit Distance: 1.25, ID: 116334_0\n",
      "Document: Aaraabi, ChromaDB Distance: 0.013513386249542236, Combined Score: 0.3844593703746796, Weighted Edit Distance: 1.25, ID: 116336_0\n",
      "Document: Arivi, ChromaDB Distance: 0.0194321870803833, Combined Score: 0.5386025309562683, Weighted Edit Distance: 1.75, ID: 119829_0\n",
      "Results for 'Aarav':\n",
      "['Aaravi', 'Aarabi', 'Aaraabi', 'Arivi']\n",
      "distance:- [-1.1920928955078125e-07, 0.013513386249542236, 0.013513386249542236, 0.0194321870803833]\n",
      "The IPA representation of 'Vivaan' is: vɪvɑn\n",
      "Results for 'Vivaan':\n",
      "[]\n",
      "distance:- []\n",
      "The IPA representation of 'Aditya' is: ɑdɪtjə\n",
      "Results for 'Aditya':\n",
      "[]\n",
      "distance:- []\n",
      "The IPA representation of 'Vihaan' is: vihɑn\n",
      "Results for 'Vihaan':\n",
      "[]\n",
      "distance:- []\n",
      "The IPA representation of 'Arjun' is: ɑɹd͡ʒən\n",
      "Results for 'Arjun':\n",
      "[]\n",
      "distance:- []\n",
      "The IPA representation of 'Sai' is: saj\n",
      "Document: Sakkh, ChromaDB Distance: 0.0, Combined Score: 0.0, Weighted Edit Distance: 0.0, ID: 78813_0\n",
      "Results for 'Sai':\n",
      "['Sakkh']\n",
      "distance:- [0.0]\n",
      "The IPA representation of 'Reyansh' is: ɹejənʃ\n",
      "Results for 'Reyansh':\n",
      "[]\n",
      "distance:- []\n",
      "The IPA representation of 'Ayaan' is: ajɑn\n",
      "Results for 'Ayaan':\n",
      "[]\n",
      "distance:- []\n",
      "The IPA representation of 'Krishna' is: kɹɪʃnə\n",
      "Results for 'Krishna':\n",
      "[]\n",
      "distance:- []\n",
      "The IPA representation of 'Ishaan' is: ɪʃɑn\n",
      "Results for 'Ishaan':\n",
      "[]\n",
      "distance:- []\n",
      "The IPA representation of 'Shaurya' is: ʃɔɹjə\n",
      "Results for 'Shaurya':\n",
      "[]\n",
      "distance:- []\n",
      "The IPA representation of 'Atharv' is: æθɑɹv\n",
      "Document: Atharvii, ChromaDB Distance: 0.0, Combined Score: 0.0, Weighted Edit Distance: 0.0, ID: 121318_0\n",
      "Document: Atharvee, ChromaDB Distance: 0.0, Combined Score: 0.0, Weighted Edit Distance: 0.0, ID: 125981_0\n",
      "Document: Atarvii, ChromaDB Distance: 0.010124385356903076, Combined Score: 0.38208706974983214, Weighted Edit Distance: 1.25, ID: 125979_0\n",
      "Results for 'Atharv':\n",
      "['Atharvii', 'Atharvee', 'Atarvii']\n",
      "distance:- [0.0, 0.0, 0.010124385356903076]\n",
      "The IPA representation of 'Dhruv' is: dɹəv\n",
      "Document: Dhieravi, ChromaDB Distance: 0.005335807800292969, Combined Score: 0.3787350654602051, Weighted Edit Distance: 1.25, ID: 132110_0\n",
      "Document: Dheeravi, ChromaDB Distance: 0.005335807800292969, Combined Score: 0.3787350654602051, Weighted Edit Distance: 1.25, ID: 132108_0\n",
      "Results for 'Dhruv':\n",
      "['Dhieravi', 'Dheeravi']\n",
      "distance:- [0.005335807800292969, 0.005335807800292969]\n",
      "The IPA representation of 'Kabir' is: kəbɹ\n",
      "Results for 'Kabir':\n",
      "[]\n",
      "distance:- []\n",
      "The IPA representation of 'Rudra' is: ɹʌdɹə\n",
      "Results for 'Rudra':\n",
      "[]\n",
      "distance:- []\n",
      "The IPA representation of 'Aarush' is: ɑɹʃ\n",
      "Document: Aarush, ChromaDB Distance: -1.1920928955078125e-07, Combined Score: -8.344650268554687e-08, Weighted Edit Distance: 0.0, ID: 866_0\n",
      "Document: Arsh, ChromaDB Distance: -1.1920928955078125e-07, Combined Score: -8.344650268554687e-08, Weighted Edit Distance: 0.0, ID: 823_0\n",
      "Document: Aarsh, ChromaDB Distance: -1.1920928955078125e-07, Combined Score: -8.344650268554687e-08, Weighted Edit Distance: 0.0, ID: 822_0\n",
      "Results for 'Aarush':\n",
      "['Aarush', 'Arsh', 'Aarsh']\n",
      "distance:- [-1.1920928955078125e-07, -1.1920928955078125e-07, -1.1920928955078125e-07]\n",
      "The IPA representation of 'Anay' is: ɑnej\n",
      "Document: Aanay, ChromaDB Distance: 1.1920928955078125e-07, Combined Score: 8.344650268554687e-08, Weighted Edit Distance: 0.0, ID: 637_0\n",
      "Document: Anay, ChromaDB Distance: 1.1920928955078125e-07, Combined Score: 8.344650268554687e-08, Weighted Edit Distance: 0.0, ID: 638_0\n",
      "Document: Ahnay, ChromaDB Distance: 1.1920928955078125e-07, Combined Score: 8.344650268554687e-08, Weighted Edit Distance: 0.0, ID: 2725_0\n",
      "Document: Aajnay, ChromaDB Distance: 1.1920928955078125e-07, Combined Score: 8.344650268554687e-08, Weighted Edit Distance: 0.0, ID: 2889_0\n",
      "Document: Anaye, ChromaDB Distance: 1.1920928955078125e-07, Combined Score: 8.344650268554687e-08, Weighted Edit Distance: 0.0, ID: 4258_0\n",
      "Document: Aunay, ChromaDB Distance: 0.010821998119354248, Combined Score: 0.23257539868354796, Weighted Edit Distance: 0.75, ID: 2729_0\n",
      "Results for 'Anay':\n",
      "['Aanay', 'Anay', 'Ahnay', 'Aajnay', 'Anaye', 'Aunay']\n",
      "distance:- [1.1920928955078125e-07, 1.1920928955078125e-07, 1.1920928955078125e-07, 1.1920928955078125e-07, 1.1920928955078125e-07, 0.010821998119354248]\n",
      "The IPA representation of 'Om' is: əm\n",
      "Document: Iham, ChromaDB Distance: 0.015616655349731445, Combined Score: 0.46093165874481196, Weighted Edit Distance: 1.5, ID: 33013_0\n",
      "Document: Addam, ChromaDB Distance: 0.015624940395355225, Combined Score: 0.16093745827674866, Weighted Edit Distance: 0.5, ID: 2081_0\n",
      "Results for 'Om':\n",
      "['Iham', 'Addam']\n",
      "distance:- [0.015616655349731445, 0.015624940395355225]\n",
      "The IPA representation of 'Parth' is: pɑɹθ\n",
      "Results for 'Parth':\n",
      "[]\n",
      "distance:- []\n",
      "The IPA representation of 'Rishi' is: ɹɪʃi\n",
      "Results for 'Rishi':\n",
      "[]\n",
      "distance:- []\n"
     ]
    }
   ],
   "source": [
    "# List of 20 Indian names\n",
    "indian_names = [\n",
    "    \"Aarav\", \"Vivaan\", \"Aditya\", \"Vihaan\", \"Arjun\",\n",
    "    \"Sai\", \"Reyansh\", \"Ayaan\", \"Krishna\", \"Ishaan\",\n",
    "    \"Shaurya\", \"Atharv\", \"Dhruv\", \"Kabir\", \"Rudra\",\n",
    "    \"Aarush\", \"Anay\", \"Om\", \"Parth\", \"Rishi\"\n",
    "]\n",
    "\n",
    "# Loop through each name, convert to IPA, and query the ChromaDB collection\n",
    "for name in indian_names:\n",
    "    ipa = english_to_ipa(name)\n",
    "    print(f\"The IPA representation of '{name}' is: {ipa}\")\n",
    "    results = querrier(ipa)\n",
    "    print(f\"Results for '{name}':\")\n",
    "    print(results['documents'])\n",
    "    # print(results['metadatas'])\n",
    "    print(\"distance:-\",results['distances'])\n",
    "indian_names = [\n",
    "    \"Aarav\", \"Vivaan\", \"Aditya\", \"Vihaan\", \"Arjun\",\n",
    "    \"Sai\", \"Reyansh\", \"Ayaan\", \"Krishna\", \"Ishaan\",\n",
    "    \"Shaurya\", \"Atharv\", \"Dhruv\", \"Kabir\", \"Rudra\",\n",
    "    \"Aarush\", \"Anay\", \"Om\", \"Parth\", \"Rishi\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fVuSKvYD9hR",
    "outputId": "5aca4918-90ad-46a5-c2c3-16236f00740b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IPA representation of 'गौतमी' is: ɡɔːtmiː\n",
      "[['Agampreet', 'Muttuk Kumaran', 'Kumbhkarna', 'Kruban', 'Pragnika']]\n",
      "[[{'age': 41, 'ipa': 'əɡəmpɹit'}, {'age': 70, 'ipa': 'mʌtək kumɹ̩æn'}, {'age': 22, 'ipa': 'kʌmbkɑɹnə'}, {'age': 37, 'ipa': 'kɹʌbən'}, {'age': 42, 'ipa': 'pɹæɡnɪkə'}]]\n",
      "[[0.8537500500679016, 0.9203306436538696, 0.9204938411712646, 0.9377778768539429, 0.9412500262260437]]\n"
     ]
    }
   ],
   "source": [
    "# Get user input\n",
    "# example :- Enter a name in Hindi:अधि\n",
    "name = input(\"Enter a name in Hindi: \")\n",
    "\n",
    "# Convert the name to IPA\n",
    "ipa = hindi_to_ipa(name)\n",
    "\n",
    "print(f\"The IPA representation of '{name}' is: {ipa}\")\n",
    "\n",
    "# Query the ChromaDB collection\n",
    "results = chromaquerrier(ipa)\n",
    "print(results['documents'])\n",
    "print(results['metadatas'])\n",
    "print(results['distances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user input\n",
    "# example :- Enter a name in Hindi:अधि\n",
    "name = input(\"Enter a name in English: \")\n",
    "\n",
    "# Convert the name to IPA\n",
    "ipa = hindi_to_ipa(name)\n",
    "\n",
    "print(f\"The IPA representation of '{name}' is: {ipa}\")\n",
    "\n",
    "# Query the ChromaDB collection\n",
    "results = chromaquerrier(ipa)\n",
    "print(results['documents'])\n",
    "print(results['metadatas'])\n",
    "print(results['distances'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
